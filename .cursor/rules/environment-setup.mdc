---
description:
globs:
alwaysApply: false
---
# Environment Setup and Dependencies

This document provides detailed setup instructions and dependency management for the project.

## System Requirements

- **OS**: Ubuntu 20.04+ (tested on 22.04)
- **Python**: 3.10 (critical for compatibility)
- **CUDA**: 12.x (for GPU support)
- **GPUs**: NVIDIA A100 recommended (40GB VRAM)
- **RAM**: 16GB+ system memory
- **Storage**: 100GB+ free space

## Critical Installation Steps

### 1. Install Miniconda

```bash
# Download and install
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3

# Add to PATH
echo 'export PATH="$HOME/miniconda3/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```

### 2. Create Conda Environment

```bash
# Create Python 3.10 environment
conda create -n faiss_env python=3.10 -y
conda init
# Close and reopen terminal
conda activate faiss_env
```

### 3. Install FAISS GPU (CRITICAL!)

⚠️ **WARNING**: This is the most critical step. Using pip-installed FAISS causes 100x slower GPU transfers!

```bash
# MUST use conda, NOT pip!
conda install -c pytorch -c nvidia faiss-gpu=1.8.0 -y

# DO NOT run: pip install faiss-gpu (causes performance issues)
```

### 4. Install PyTorch with CUDA

```bash
# For CUDA 12.1
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```

### 5. Install Python Dependencies

```bash
cd /home/ubuntu/SPEEDINGTHEPROCESS/old_app

# Install requirements
pip install -r requirements.txt

# CRITICAL: Downgrade transformers for GME compatibility
pip install transformers==4.51.3
```

## Model Files Setup

### GME Base Model

The GME-Qwen2-VL-7B-Instruct model should be in:
```
gme-Qwen2-VL-7B-Instruct/
├── config.json
├── model.safetensors
├── tokenizer.json
└── ...
```

### LoRA Checkpoints

LoRA adapters should be in:
```
loras/
└── v11-20250620-105815/
    └── checkpoint-1095/
        ├── adapter_config.json
        └── adapter_model.bin
```

## Data Files Setup

### Required Files

1. **Product Database**:
   ```
   database_results/
   └── final_with_aws_shapes_enriched.csv  # 34,431 products
   ```

2. **FAISS Indexes**:
   ```
   indexes/
   ├── v11_complete_merged_20250625_115302.faiss
   ├── v11_complete_merged_20250625_115302_embeddings.npy
   └── v11_complete_merged_20250625_115302_metadata.json
   ```

3. **Product Images**:
   ```
   pictures/
   ├── 2087270FMP53HA_O00.jpg
   ├── 2087270FMP53HA.jpg
   └── ... (29,104 images)
   ```

## Environment Variables

### GPU Configuration

```bash
# Enable/disable GPU
export USE_FAISS_GPU=true

# CUDA optimizations
export CUDA_CACHE_MAXSIZE=2147483647
export CUDA_CACHE_DISABLE=0

# Threading optimizations (prevents conflicts)
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export TOKENIZERS_PARALLELISM=false
```

### Memory Management

```bash
# Reduce CUDA memory fragmentation
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

## Dependency Versions

Critical versions from [requirements.txt](mdc:requirements.txt):

```
torch>=1.9.0              # PyTorch for deep learning
transformers==4.51.3      # MUST be this version for GME
faiss-gpu>=1.7.0         # MUST be conda-installed
fastapi>=0.68.0          # Web framework
pandas>=1.3.0            # Data processing
numpy>=1.21.0            # Numerical computing
pillow>=8.0.0            # Image processing
```

## Startup Scripts

### [start_gpu.sh](mdc:start_gpu.sh)
```bash
#!/bin/bash
# Activates conda environment and starts FastAPI server
conda activate faiss_env
export USE_FAISS_GPU=true
uvicorn app:app --host 127.0.0.1 --port 8080 --reload
```

### [start_openclip_gpu.sh](mdc:start_openclip_gpu.sh)
```bash
# Starts OpenCLIP variant
uvicorn app_openclip:app --host 127.0.0.1 --port 8080
```

## Common Issues and Solutions

### Issue: "FAISS GPU transfer taking minutes"
**Solution**: Reinstall FAISS with conda (see step 3)

### Issue: "Model loading fails"
**Solution**: Check transformers version is exactly 4.51.3

### Issue: "CUDA out of memory"
**Solution**: 
- Reduce batch size
- Enable Float16 precision
- Clear GPU cache

### Issue: "Module not found"
**Solution**: Ensure conda environment is activated

## Performance Verification

After setup, verify performance:

```bash
# Test FAISS GPU
python test_faiss_gpu_performance.py

# Check GPU status
python check_gpu_status.py

# Monitor GPU usage
watch -n 1 nvidia-smi
```

Expected results:
- GPU transfer: 2-5 seconds (not minutes!)
- Search speed: 10-50x faster than CPU
- All 4 GPUs should be visible

## Development Tools

Recommended VSCode extensions:
- Python
- Pylance
- Jupyter
- GitLens

Debugging:
```python
# Add to code for GPU memory tracking
from faiss_gpu_utils import faiss_gpu_manager
print(faiss_gpu_manager.get_memory_usage())
```
